<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>MAR Workshop 2024</title>
    <style>
        body {
            font-family: Arial, sans-serif;
            margin: 40px auto;
            max-width: 800px;
            line-height: 1.6;
            color: #333;
            background-color: #f4f4f4;
            padding: 20px;
            border-radius: 8px;
            box-shadow: 0 0 10px rgba(0,0,0,0.1);
        }
        h1, h2 {
            color: #333;
        }
        .authors {
            margin-bottom: 20px;
            color: #555;
        }
        .abstract {
            background-color: #fff;
            padding: 15px;
            border-left: 5px solid #007BFF;
            margin: 20px 0;
        }
        .graphics {
            text-align: center;
            margin: 20px 0;
        }
        .acknowledgements, .dataset {
            text-align: center;
            margin: 20px 0;
        }
    </style>
</head>
<body>
    <h1>Using Language-Aligned Gesture Embeddings for Understanding Gestures Accompanying Math Terms</h1>
    <div class="authors">
        <p>Tristan Maidment, Purav J Patel, Erin Walker, Adriana Kovashka<br>
    </div>
    <div class="abstract">
        <h2>Abstract</h2>
        <p>In this paper, we introduce an approach for recognizing and classifying gestures that accompany mathematical terms, in a new collection we name the “GAMT” dataset. Our method uses language as a means of providing context to classify gestures. Specifically, we use a CLIP-style framework to construct a shared embedding space for gestures and language, experimenting with various methods for encoding gestures within this space. We evaluate our method on our new dataset containing a wide array of gestures associated with mathematical terms. The shared embedding space leads to a substantial improvement in gesture classification. Furthermore, we identify an efficient model that excelled at classifying gestures from our unique dataset, thus contributing to the further development of gesture recognition in diverse interaction scenarios.</p>
    </div>

    <div class="graphics">
        <h2>Dataset</h2>
        
        <img src="images/dataset_example_large.png" alt="dataset example" style="max-width:60%;">
        <p>Dataset Available Soon</p>
    <div class="acknowledgements">
        <p>This work was supported by Grant No. 2024645 from the National Science Foundation.</p>
    </div>
    </div>
</body>
</html>
